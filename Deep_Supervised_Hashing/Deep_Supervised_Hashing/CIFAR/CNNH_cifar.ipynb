{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 10, 'H_save_path': 'save/CNNH/', 'optimizer': {'type': <class 'torch.optim.adam.Adam'>, 'optim_params': {'lr': 1e-05, 'betas': (0.9, 0.999)}}, 'info': '[CNNH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class 'network_cifar.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 150, 'test_map': 10, 'device': device(type='cuda', index=1), 'bit_list': [48], 'topK': -1, 'n_class': 10, 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}}\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /dataset/cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:34<00:00, 1798159.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /dataset/cifar/cifar-10-python.tar.gz to /dataset/cifar/\n",
      "train_dataset 5000\n",
      "test_dataset 1000\n",
      "database_dataset 59000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\maopy/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:13<00:00, 18.6MB/s] \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbit_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 155\u001b[0m     \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 103\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(config, bit)\u001b[0m\n\u001b[0;32m    101\u001b[0m train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset \u001b[38;5;241m=\u001b[39m get_data(config)\n\u001b[0;32m    102\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_train\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_train\n\u001b[1;32m--> 103\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbit\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# get database_labels\u001b[39;00m\n\u001b[0;32m    106\u001b[0m clses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from network_cifar import *\n",
    "from tools_cifar import *\n",
    "\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# CNNH(AAAI2014)\n",
    "# paper [Supervised Hashing for Image Retrieval via Image Representation Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8137/8861)\n",
    "# code[CNNH-pytorch](https://github.com/heheqianqian/CNNH)\n",
    "# [CNNH] epoch:20, bit:48, dataset:cifar10-1, MAP:0.134, Best MAP: 0.134\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    config = {\n",
    "        \"T\": 10,\n",
    "        \"H_save_path\": \"save/CNNH/\",\n",
    "        \"optimizer\": {\"type\": optim.Adam, \"optim_params\": {\"lr\": 1e-5, \"betas\": (0.9, 0.999)}},\n",
    "        \"info\": \"[CNNH]\",\n",
    "        \"resize_size\": 256,\n",
    "        \"crop_size\": 224,\n",
    "        \"batch_size\": 128,\n",
    "        \"net\": AlexNet,\n",
    "        # \"net\":ResNet,\n",
    "        \"dataset\": \"cifar10-1\",\n",
    "        # \"dataset\": \"pathmnist\",\n",
    "        \"epoch\": 150,\n",
    "        \"test_map\": 10,\n",
    "        # \"device\":torch.device(\"cpu\"),\n",
    "        \"device\": torch.device(\"cuda:1\"),\n",
    "        \"bit_list\": [48],\n",
    "    }\n",
    "    config = config_dataset(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "class CNNHLoss(torch.nn.Module):\n",
    "    def __init__(self, config, train_labels, bit):\n",
    "\n",
    "        super(CNNHLoss, self).__init__()\n",
    "        S = (train_labels @ train_labels.t() > 0).float() * 2 - 1\n",
    "        # load H if exists\n",
    "        save_full_path = \"%sH_T(%d)_bit(%d)_dataset(%s).pt\" % (\n",
    "            config[\"H_save_path\"], config[\"T\"], bit, config[\"dataset\"])\n",
    "        if os.path.exists(save_full_path):\n",
    "            print(\"loading \", save_full_path)\n",
    "            self.H = torch.load(save_full_path).to(config[\"device\"])\n",
    "        else:\n",
    "            self.H = self.stage_one(config[\"num_train\"], bit, config[\"T\"], S, config[\"H_save_path\"], config[\"dataset\"],\n",
    "                                    config[\"device\"])\n",
    "\n",
    "    def stage_one(self, n, q, T, S, H_save_path, dataset, device):\n",
    "\n",
    "        if not os.path.exists(H_save_path):\n",
    "            os.makedirs(H_save_path)\n",
    "\n",
    "        H = 2 * torch.rand((n, q)).to(device) - 1\n",
    "        L = H @ H.t() - q * S\n",
    "        permutation = list(product(range(n), range(q)))\n",
    "        for t in range(T):\n",
    "            H_temp = H.clone()\n",
    "            L_temp = L.clone()\n",
    "            shuffle(permutation)\n",
    "            for i, j in tqdm(permutation):\n",
    "                # formula 7\n",
    "                g_prime_Hij = 4 * L[i, :] @ H[:, j]\n",
    "                g_prime_prime_Hij = 4 * (H[:, j].t() @ H[:, j] + H[i, j].pow(2) + L[i, i])\n",
    "                # formula 6\n",
    "                d = (-g_prime_Hij / g_prime_prime_Hij).clamp(min=-1 - H[i, j], max=1 - H[i, j])\n",
    "                # formula 8\n",
    "                L[i, :] = L[i, :] + d * H[:, j].t()\n",
    "                L[:, i] = L[:, i] + d * H[:, j]\n",
    "                L[i, i] = L[i, i] + d * d\n",
    "\n",
    "                H[i, j] = H[i, j] + d\n",
    "\n",
    "            if L.pow(2).mean() >= L_temp.pow(2).mean():\n",
    "                H = H_temp\n",
    "                L = L_temp\n",
    "            save_full_path = \"%sH_T(%d)_bit(%d)_dataset(%s).pt\" % (H_save_path, t + 1, bit, dataset)\n",
    "            torch.save(H.sign().cpu(), save_full_path)\n",
    "            print(\"[CNNH stage 1][%d/%d] reconstruction loss:%.7f ,H save in %s\" % (\n",
    "                t + 1, T, L.pow(2).mean().item(), save_full_path))\n",
    "        return H.sign()\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        loss = (u - self.H[ind]).pow(2).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_val(config, bit):\n",
    "    device = config[\"device\"]\n",
    "    train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
    "    config[\"num_train\"] = num_train\n",
    "    net = config[\"net\"](bit).to(device)\n",
    "\n",
    "    # get database_labels\n",
    "    clses = []\n",
    "    for _, cls, _ in tqdm(train_loader):\n",
    "        clses.append(cls)\n",
    "    train_labels = torch.cat(clses).to(device).float()\n",
    "\n",
    "    optimizer = config[\"optimizer\"][\"type\"](net.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "\n",
    "    print(\"Stage 1: learning approximate hash codes.\")\n",
    "    criterion = CNNHLoss(config, train_labels, bit)\n",
    "    print(\"Stage 2: learning images feature representation and hash functions.\")\n",
    "\n",
    "    Best_mAP = 0\n",
    "\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = net(image)\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, config)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.3f\" % (train_loss))\n",
    "\n",
    "        if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "            Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = get_config()\n",
    "    print(config)\n",
    "    for bit in config[\"bit_list\"]:\n",
    "        train_val(config, bit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKogSysMLB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
