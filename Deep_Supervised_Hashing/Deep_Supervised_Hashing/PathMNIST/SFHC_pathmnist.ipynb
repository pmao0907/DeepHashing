{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[SFHC]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 64, 'net': <class '__main__.SFHC'>, 'dataset': 'pathmnist', 'epoch': 250, 'test_map': 15, 'save_path': 'save/SFHC', 'device': device(type='cuda', index=1), 'bit_list': [48], 'topK': -1, 'n_class': 9, 'data': {'train_set': {'list_path': './data/pathmnist/train.txt', 'batch_size': 64}, 'database': {'list_path': './data/pathmnist/database.txt', 'batch_size': 64}, 'test': {'list_path': './data/pathmnist/test.txt', 'batch_size': 64}}}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to setup the default `root` directory. Please specify and create the `root` directory manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 192\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbit_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr_curve_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog/alexnet/SFHC_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 146\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(config, bit)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_val\u001b[39m(config, bit):\n\u001b[0;32m    145\u001b[0m     device \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 146\u001b[0m     train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_train\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_train\n\u001b[0;32m    148\u001b[0m     net \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m](bit)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\maopy\\Desktop\\BachelorThesis\\Deep_Supervised_Hashing\\Deep_Supervised_Hashing\\PathMNIST\\tools_pathmnist.py:210\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cifar_dataset(config)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpathmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpathmnist_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m dsets \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    213\u001b[0m dset_loaders \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\maopy\\Desktop\\BachelorThesis\\Deep_Supervised_Hashing\\Deep_Supervised_Hashing\\PathMNIST\\tools_pathmnist.py:173\u001b[0m, in \u001b[0;36mpathmnist_dataset\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    166\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    167\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    168\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    169\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m    170\u001b[0m ])\n\u001b[0;32m    171\u001b[0m pathmnist_dataset_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dataset/pathmnist/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 173\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMyPathMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpathmnist_dataset_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MyPathMNIST(root\u001b[38;5;241m=\u001b[39mpathmnist_dataset_root,\n\u001b[0;32m    179\u001b[0m                          split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    180\u001b[0m                          transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    181\u001b[0m                          download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    183\u001b[0m database_dataset \u001b[38;5;241m=\u001b[39m MyPathMNIST(root\u001b[38;5;241m=\u001b[39mpathmnist_dataset_root,\n\u001b[0;32m    184\u001b[0m                              split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    185\u001b[0m                              transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    186\u001b[0m                              download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maopy\\anaconda3\\envs\\envKogSysMLB\\Lib\\site-packages\\medmnist\\dataset.py:31\u001b[0m, in \u001b[0;36mMedMNIST.__init__\u001b[1;34m(self, split, transform, target_transform, download, as_rgb, root)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to setup the default `root` directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     32\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify and create the `root` directory manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to setup the default `root` directory. Please specify and create the `root` directory manually."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from network_pathmnist import *\n",
    "from tools_pathmnist import *\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "# Paper: Simultaneous Feature Learning and Hash Coding with Deep Neural Networks\n",
    "# https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf\n",
    "# 1) a sub-network with multiple convolution-pooling layers to capture a representation of images; \n",
    "# 2) a divide-and-encode module designed to generate bitwise hash codes; \n",
    "# 3) a triplet ranking loss layer for learning good similarity measures. \n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        # Define the convolution-pooling layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the convolution-pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # Reshape the features for the hashing module\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DivideAndEncode(nn.Module):\n",
    "    def __init__(self, input_dim, hash_bits, epsilon=0.01, beta=10):\n",
    "        super(DivideAndEncode, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hash_bits = hash_bits\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        # Define fully connected layers\n",
    "        self.fc = nn.Linear(input_dim, hash_bits)\n",
    "        # Define sigmoid activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = self.fc(x)\n",
    "        # Apply sigmoid activation\n",
    "        x = self.sigmoid(x * self.beta)\n",
    "        # Apply piece-wise threshold function\n",
    "        x = self.piecewise_threshold(x)\n",
    "        return x\n",
    "\n",
    "    def piecewise_threshold(self, x):\n",
    "        # Piece-wise threshold function\n",
    "        thresholded_x = torch.zeros_like(x)\n",
    "        thresholded_x[x >= 0.5 + self.epsilon] = 1\n",
    "        thresholded_x[x <= 0.5 - self.epsilon] = 0\n",
    "        thresholded_x[(x > 0.5 - self.epsilon) & (x < 0.5 + self.epsilon)] = x[(x > 0.5 - self.epsilon) & (x < 0.5 + self.epsilon)]\n",
    "        return thresholded_x\n",
    "\n",
    "class SFHC(nn.Module):\n",
    "    def __init__(self, hash_bits):\n",
    "        super(SFHC, self).__init__()\n",
    "        # Create shared sub-network and divide-and-encode modules\n",
    "        self.shared_subnetwork = SubNetwork()\n",
    "        self.hashing_module = DivideAndEncode(128 * 8 * 8, hash_bits)\n",
    "\n",
    "    def forward(self, x_anchor, x_positive, x_negative):\n",
    "        # Forward pass through the shared sub-network\n",
    "        x_anchor = self.shared_subnetwork(x_anchor)\n",
    "        x_positive = self.shared_subnetwork(x_positive)\n",
    "        x_negative = self.shared_subnetwork(x_negative)\n",
    "        # Forward pass through the divide-and-encode module\n",
    "        hash_code_anchor = self.hashing_module(x_anchor)\n",
    "        hash_code_positive = self.hashing_module(x_positive)\n",
    "        hash_code_negative = self.hashing_module(x_negative)\n",
    "        return hash_code_anchor, hash_code_positive, hash_code_negative\n",
    "    \n",
    "\n",
    "def get_config():\n",
    "    config = {\n",
    "        \"alpha\": 0.1,\n",
    "        # \"optimizer\":{\"type\":  optim.SGD, \"optim_params\": {\"lr\": 0.05, \"weight_decay\": 10 ** -5}},\n",
    "        \"optimizer\": {\"type\": optim.RMSprop, \"optim_params\": {\"lr\": 1e-5, \"weight_decay\": 10 ** -5}},\n",
    "        \"info\": \"[SFHC]\",\n",
    "        \"resize_size\": 256,\n",
    "        \"crop_size\": 224,\n",
    "        \"batch_size\": 64,\n",
    "        \"net\": SFHC,\n",
    "        # \"net\":ResNet,\n",
    "        # \"dataset\": \"cifar10\",\n",
    "        # \"dataset\": \"cifar10-1\",\n",
    "        \"dataset\": \"pathmnist\",\n",
    "        \"epoch\": 250,\n",
    "        \"test_map\": 15,\n",
    "        \"save_path\": \"save/SFHC\",\n",
    "        # \"device\":torch.device(\"cpu\"),\n",
    "        \"device\": torch.device(\"cuda:1\"),\n",
    "        \"bit_list\": [48],\n",
    "    }\n",
    "    config = config_dataset(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def train_SFHC(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output_anchor, output_positive, output_negative = model(data, data, data)\n",
    "        # Compute the loss\n",
    "        loss = criterion(output_anchor, output_positive, output_negative)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "class SFHCLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(SFHCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, b, b_pos, b_neg):\n",
    "        # compute the L2 norms of the diffs\n",
    "        diff_pos = torch.norm(b - b_pos, p=2, dim=1) ** 2\n",
    "        diff_neg = torch.norm(b - b_neg, p=2, dim=1) ** 2\n",
    "\n",
    "        # compute the triplet ranking hinge loss\n",
    "        loss = torch.max(0, diff_pos - diff_neg + 1)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def train_val(config, bit):\n",
    "    device = config[\"device\"]\n",
    "    train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
    "    config[\"num_train\"] = num_train\n",
    "    net = config[\"net\"](bit).to(device)\n",
    "\n",
    "    optimizer = config[\"optimizer\"][\"type\"](net.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "\n",
    "    criterion = SFHCLoss(config, bit)\n",
    "\n",
    "    Best_mAP = 0\n",
    "\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = net(image)\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, config)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.3f\" % (train_loss))\n",
    "\n",
    "        if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "            Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = get_config()\n",
    "    print(config)\n",
    "    for bit in config[\"bit_list\"]:\n",
    "        config[\"pr_curve_path\"] = f\"log/alexnet/SFHC_{config['dataset']}_{bit}.json\"\n",
    "        train_val(config, bit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKogSysMLB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
