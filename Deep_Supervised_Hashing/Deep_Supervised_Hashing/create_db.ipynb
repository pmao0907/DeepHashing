{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import timm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_dataloaders(dataset_name, transform, batch_size=32, num_workers=4, download=True):    \n",
    "    if dataset_name not in ['cifar10', 'cifar100']:\n",
    "        raise ValueError(\"Dataset must be 'cifar10' or 'cifar100'\")\n",
    "    \n",
    "    dataset_class = datasets.CIFAR10 if dataset_name == 'cifar10' else datasets.CIFAR100\n",
    "    \n",
    "    train_dataset = dataset_class(root='./data', train=True, download=download, transform=transform)\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=download, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def extract_embeddings(model, device, dataloader):\n",
    "\n",
    "    embeddings_db, labels_db = [], []\n",
    "\n",
    "    for extracted in tqdm(dataloader):\n",
    "        images, labels = extracted\n",
    "        images = images.to(device)\n",
    "        output = model.forward_features(images)\n",
    "        output = model.forward_head(output, pre_logits=True)\n",
    "        labels_db.extend(labels)\n",
    "        embeddings_db.extend(output.detach().cpu().numpy())\n",
    "\n",
    "    data = {\n",
    "        'embeddings': embeddings_db,\n",
    "        'labels': labels_db\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(dataset, backbone, seed=42):\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # get model from timm\n",
    "    model = timm.create_model(backbone, pretrained=True, num_classes=0).to(device)\n",
    "    model.requires_grad_(False) # remove?\n",
    "    model = model.eval()\n",
    "    \n",
    "    # get the required transform function for the given feature extractor\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    # get dataloaders and filenames\n",
    "    dataloaders  = get_dataloaders(dataset, transforms)\n",
    "\n",
    "    # create database folders, if necessary\n",
    "    os.makedirs(dataset, exist_ok=True)\n",
    "\n",
    "    for split in ['train','test']:\n",
    "\n",
    "        # get database of embeddings in the form\n",
    "        #   db = {'embeddings' : [...], 'labels' : [...], \n",
    "        # the filenames are used for explainability purposes    \n",
    "        db = extract_embeddings( model = model, \n",
    "                                 device = device,\n",
    "                                 dataloader = dataloaders[split])\n",
    "        \n",
    "        # store database\n",
    "        # database_root / dataset / train|test.npz\n",
    "        np.savez(os.path.join(dataset,f'{split}.npz'), **db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"vit_base_patch14_dinov2.lvd142m\"\n",
    "dataset = \"cifar10\" # or cifar100\n",
    "\n",
    "\n",
    "create_database(dataset, backbone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
