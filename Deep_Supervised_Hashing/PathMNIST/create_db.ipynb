{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmao/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from medmnist import INFO, Evaluator\n",
    "from medmnist import PathMNIST\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import timm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_dataloaders(dataset_name, transform, batch_size=32, num_workers=4, download=True):    \n",
    "    if dataset_name not in ['pathmnist']:\n",
    "        raise ValueError(\"Dataset must be 'pathmnist'\")\n",
    "    \n",
    "    dataset_class = PathMNIST\n",
    "    \n",
    "    train_dataset = PathMNIST(split=\"train\", download=True, transform=transform)\n",
    "    test_dataset = PathMNIST(split=\"test\", download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def extract_embeddings(model, device, dataloader):\n",
    "\n",
    "    embeddings_db, labels_db = [], []\n",
    "\n",
    "    for extracted in tqdm(dataloader):\n",
    "        images, labels = extracted\n",
    "        images = images.to(device)\n",
    "        output = model.forward_features(images)\n",
    "        output = model.forward_head(output, pre_logits=True)\n",
    "        labels_db.extend(labels)\n",
    "        embeddings_db.extend(output.detach().cpu().numpy())\n",
    "\n",
    "    data = {\n",
    "        'embeddings': embeddings_db,\n",
    "        'labels': labels_db\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(dataset, backbone, save_dir, seed=42):\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # get model from timm\n",
    "    model = timm.create_model(backbone, pretrained=True, num_classes=0).to(device)\n",
    "    model.requires_grad_(False) # remove?\n",
    "    model = model.eval()\n",
    "    \n",
    "    # get the required transform function for the given feature extractor\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    # get dataloaders and filenames\n",
    "    dataloaders  = get_dataloaders(dataset, transforms)\n",
    "\n",
    "\n",
    "    full_save_path = os.path.join(save_dir, dataset)\n",
    "    # create database folders, if necessary\n",
    "    os.makedirs(full_save_path, exist_ok=True)\n",
    "\n",
    "    for split in ['train','test']:\n",
    "\n",
    "        # get database of embeddings in the form\n",
    "        #   db = {'embeddings' : [...], 'labels' : [...], \n",
    "        # the filenames are used for explainability purposes    \n",
    "        db = extract_embeddings( model = model, \n",
    "                                 device = device,\n",
    "                                 dataloader = dataloaders[split])\n",
    "        \n",
    "        # store database\n",
    "        # database_root / dataset / train|test.npz\n",
    "        np.savez(os.path.join(full_save_path,f'{split}.npz'), **db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"vit_base_patch14_dinov2.lvd142m\"\n",
    "dataset = \"pathmnist\"\n",
    "save_dir = \"/home/pmao/BachelorThesis/Deep_supervised_Hashing/PathMNIST/MedMNIST-PathMNIST\"\n",
    "\n",
    "create_database(dataset, backbone, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deephashing1",
   "language": "python",
   "name": "deephashing1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
