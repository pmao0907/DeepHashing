{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nearpy import Engine\n",
    "from nearpy.hashes import RandomBinaryProjections  # other hash functions can be chosen\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LSH Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dimensions = 2048 \n",
    "num_tables = 10 \n",
    "hashes = [RandomBinaryProjections('rbp', 10) for _ in range(num_tables)]  # Adjust number of hash functions\n",
    "engine = Engine(num_dimensions, lshashes=hashes)\n",
    "\n",
    "embeddings_train, labels_train = np.load('pathmnist/train.npz')['embeddings'], np.load('pathmnist/train.npz')['labels']\n",
    "embeddings_val, labels_val = np.load('pathmnist/val.npz')['embeddings'], np.load('pathmnist/val.npz')['labels']\n",
    "embeddings_test, labels_test = np.load('pathmnist/test.npz')['embeddings'], np.load('pathmnist/test.npz')['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add embeddings to LSH engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, embedding in enumerate(embeddings_train):\n",
    "    engine.store_vector(embedding, labels_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query LSH engine for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = embeddings_val\n",
    "true_labels = labels_val\n",
    "predicted_labels = []\n",
    "\n",
    "# Evaluate accuracy and mAP\n",
    "correct_count = 0\n",
    "precisions = []\n",
    "recalls = []\n",
    "average_precisions = []\n",
    "\n",
    "for i, query_embedding in enumerate(query_embeddings):\n",
    "    neighbors = engine.neighbours(query_embedding)\n",
    "\n",
    "    # Retrieve labels of neighbors\n",
    "    neighbor_labels = [int(neighbor[1].split('_')[1]) for neighbor in neighbors]\n",
    "    \n",
    "    # Predict label based on majority voting among neighbors\n",
    "    predicted_label = max(set(neighbor_labels), key=neighbor_labels.count)\n",
    "    predicted_labels.append(predicted_label)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if predicted_label == true_labels[i]:\n",
    "        correct_count += 1\n",
    "    \n",
    "    # Calculate precision and recall for this query\n",
    "    true_positive = len(set(neighbor_labels) & set([true_labels[i]]))\n",
    "    precision = true_positive / len(neighbor_labels)\n",
    "    recall = true_positive / 1  # Because we are retrieving only 1 neighbor\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n",
    "    # Calculate average precision for this query\n",
    "    average_precisions.append(average_precision_score([true_labels[i]], [predicted_label]))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_count / len(query_embeddings)\n",
    "\n",
    "# Calculate mean precision, recall, and mAP\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mAP = np.mean(average_precisions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "print(\"mAP:\", mAP)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
